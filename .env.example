# LLM Configuration
# MODEL_NAME should be set to the name of your locally hosted Ollama model
MODEL_NAME=
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Search API Keys
TAVILY_API_KEY=your_tavily_api_key_here
BRAVE_API_KEY=your_brave_api_key_here

# Optional GPU Settings
GPU_ENABLED=false
GPU_MAX_LAYERS=-1
GPU_MEMORY_LIMIT=0